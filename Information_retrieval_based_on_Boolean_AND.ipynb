{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Information retrieval based on Boolean AND",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dDJMQvHkQPE"
      },
      "source": [
        "In  this  project,  I am  building  a  simple  Information  Retrieval  system  based  on  Boolean  AND operation.   The  main  idea  is  to  create  a  fast  retrieval  system  using  inverse  indexing  and create relevant posting list.\n",
        "\n",
        "The first part of the code is to load the dataset. If we are using google colab, we authorise the access by google account and change to the google drive where the dataset resides. I am making use of the nltk package for stemmer and lemmatization of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucOGK2qPwPRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cfea099-0daa-441d-b72a-a0408d81e4d2"
      },
      "source": [
        "import sys, os\n",
        "import nltk\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    os.chdir('/content/gdrive')\n",
        "\n",
        "#Download stemmer (punkt) and lemmatiser (wordnet)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#Using SnowballStemmer: Reason -> https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer=SnowballStemmer('english')\n",
        "\n",
        "#Using WordNet Lemmatiser\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "#Load stopwords\n",
        "## Corrected stopword file name\n",
        "for p,d,f in os.walk('.'):\n",
        "  if 'stopwords.txt' in f:\n",
        "    print(f)\n",
        "    fullpath=os.path.join(p,'stopwords.txt')\n",
        "    break\n",
        "fstop=open(fullpath,encoding='utf-8-sig')\n",
        "stopw=fstop.read().split()\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "['Proj2.ipynb', 'stopwords.txt', 'Proj3-benchmark.ipynb']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJUhHiOEpPKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "854ed7af-a903-40c0-914f-5703a2dbad10"
      },
      "source": [
        "#Stemmer test\n",
        "a=stemmer.stem('transferring')\n",
        "print(a)\n",
        "#Lemma test\n",
        "a=lemmatizer.lemmatize('feet')\n",
        "print(a)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "transfer\n",
            "foot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wdI-pZXlOVM"
      },
      "source": [
        "The following function searches the specified dataset folder name and returns the full directory path of the dataset, \"fullpath\" with a list of filename in a list, \"docfilename\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8sMQgj-k9Cn"
      },
      "source": [
        "def directory_list(datasetname):\n",
        "  fullpath=''\n",
        "  for p,d,f in os.walk('.'):\n",
        "    if datasetname in d:\n",
        "      fullpath=os.path.join(p,datasetname)\n",
        "      break\n",
        "  os.chdir(fullpath)\n",
        "  docfilename=os.listdir()\n",
        "  return fullpath,docfilename"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDPgYgGTmCGf"
      },
      "source": [
        "The following searches the dataset named \"HillaryEmails\" and return full path \"path\" to the dataset and a list of documents text \"docfilename\" in the directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ElcaE2Oxj_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56482fda-31d6-40a8-ab29-e527f00c4a07"
      },
      "source": [
        "path,docfilename=directory_list('HillaryEmails')\n",
        "print(\"The path name is: \", path)\n",
        "print(\"The list of documents in the dataset: \", docfilename)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The path name is:  ./MyDrive/Colab Notebooks/HillaryEmails\n",
            "The list of documents in the dataset:  ['1053.gdoc', '5547.gdoc', '7422.gdoc', '3644.gdoc', '7436.gdoc', '4881.gdoc', '1735.gdoc', '5235.gdoc', '4895.gdoc', '7344.gdoc', '3122.gdoc', '5553.gdoc', '4659.gdoc', '7350.gdoc', '1721.gdoc', '2228.gdoc', '6728.gdoc', '1047.gdoc', '3650.gdoc', '3136.gdoc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDR5z6hy5Dnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc3f2a1d-87b2-4b9e-9416-22a2e213a9ba"
      },
      "source": [
        "docidlen=len(docfilename)\n",
        "print(\"The number of documents in the dataset: \", docidlen)\n",
        "for i,d in enumerate(docfilename):\n",
        "  print(i, d)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of documents in the dataset:  20\n",
            "0 1053.gdoc\n",
            "1 5547.gdoc\n",
            "2 7422.gdoc\n",
            "3 3644.gdoc\n",
            "4 7436.gdoc\n",
            "5 4881.gdoc\n",
            "6 1735.gdoc\n",
            "7 5235.gdoc\n",
            "8 4895.gdoc\n",
            "9 7344.gdoc\n",
            "10 3122.gdoc\n",
            "11 5553.gdoc\n",
            "12 4659.gdoc\n",
            "13 7350.gdoc\n",
            "14 1721.gdoc\n",
            "15 2228.gdoc\n",
            "16 6728.gdoc\n",
            "17 1047.gdoc\n",
            "18 3650.gdoc\n",
            "19 3136.gdoc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa4Saxa8n6MW"
      },
      "source": [
        "The following function \"processString\" processes an input string by removing punctuations, lowering the case, removing stop words, lemmatising and stemming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3miDRV_o5bQ"
      },
      "source": [
        "  def processString(String, stopw):\n",
        "\n",
        "      global orisz\n",
        "      orisz+=sys.getsizeof(String)\n",
        "\n",
        "      #Removes symbols completely\n",
        "      for i in ',!@#$%^&*()-=+_\":\\';<>/\\\\.?':\n",
        "          String=String.replace(i,'')\n",
        "\n",
        "      if bench:\n",
        "          global rmpuncsz\n",
        "          rmpuncsz+=sys.getsizeof(String)\n",
        "\n",
        "      #Convert all to lowercase\n",
        "      String=String.lower()\n",
        "\n",
        "      #Remove stop words (https://code.google.com/archive/p/stop-words/)\n",
        "      ##Split words into a list with separate entries (separated by space)\n",
        "      String=String.split()\n",
        "      for s in stopw:\n",
        "          for i in range(len(String)-1,-1,-1):\n",
        "              if String[i]==s:\n",
        "                  String.pop(i)\n",
        "\n",
        "      if bench:\n",
        "          global stopsz\n",
        "          stopsz+=sys.getsizeof(' '.join(String))\n",
        "\n",
        "          global stemdur\n",
        "          #Using snowball stemmer\n",
        "          temp=time.time()\n",
        "          S0=[stemmer.stem(i) for i in String]\n",
        "          stemdur[0]+=time.time()-temp\n",
        "\n",
        "          #Porter Stemmer\n",
        "          temp=time.time()\n",
        "          S1=[porter.stem(i) for i in String]\n",
        "          stemdur[1]+=time.time()-temp\n",
        "\n",
        "          #Lancaster Stemmer\n",
        "          temp=time.time()\n",
        "          S2=[lancaster.stem(i) for i in String]\n",
        "          stemdur[2]+=time.time()-temp\n",
        "\n",
        "          global stemsz\n",
        "          str1=' '.join(S0)\n",
        "          str2=' '.join(S1)\n",
        "          str3=' '.join(S2)\n",
        "          stemsz[0]+=sys.getsizeof(str1)\n",
        "          stemsz[1]+=sys.getsizeof(str2)\n",
        "          stemsz[2]+=sys.getsizeof(str3)\n",
        "          \n",
        "          global lemmadur\n",
        "\n",
        "          #Using wordnet lemmatiser\n",
        "          temp=time.time()\n",
        "          S00=[lemmatizer.lemmatize(i) for i in S0]\n",
        "          lemmadur[0,0]+=time.time()-temp\n",
        "\n",
        "          temp=time.time()\n",
        "          S10=[lemmatizer.lemmatize(i) for i in S1]\n",
        "          lemmadur[1,0]+=time.time()-temp\n",
        "\n",
        "          temp=time.time()\n",
        "          S20=[lemmatizer.lemmatize(i) for i in S2]\n",
        "          lemmadur[2,0]+=time.time()-temp\n",
        "          \n",
        "          #Using SpaCy lemmatizer\n",
        "          temp=time.time()\n",
        "          S01=' '.join([token.lemma_ for token in spacymod(str1)]) \n",
        "          lemmadur[0,1]+=time.time()-temp\n",
        "\n",
        "          temp=time.time()\n",
        "          S11=' '.join([token.lemma_ for token in spacymod(str2)])\n",
        "          lemmadur[1,1]+=time.time()-temp\n",
        "          \n",
        "          temp=time.time()\n",
        "          S21=' '.join([token.lemma_ for token in spacymod(str3)])\n",
        "          lemmadur[2,1]+=time.time()-temp\n",
        "\n",
        "          #Using TextBlob lemmatizer\n",
        "          temp=time.time()\n",
        "          S02=\" \".join([w.lemmatize() for w in TextBlob(str1).words])\n",
        "          lemmadur[0,2]+=time.time()-temp\n",
        "\n",
        "          temp=time.time()\n",
        "          S12=\" \".join([w.lemmatize() for w in TextBlob(str2).words])\n",
        "          lemmadur[1,2]+=time.time()-temp\n",
        "\n",
        "          temp=time.time()\n",
        "          S22=\" \".join([w.lemmatize() for w in TextBlob(str3).words])\n",
        "          lemmadur[2,2]+=time.time()-temp\n",
        "\n",
        "          String=' '.join(S00)\n",
        "          S1=' '.join(S10)\n",
        "          S2=' '.join(S20)\n",
        "          global lemmasz\n",
        "\n",
        "          lemmasz[0,0]+=sys.getsizeof(String)\n",
        "          lemmasz[1,0]+=sys.getsizeof(S1)\n",
        "          lemmasz[2,0]+=sys.getsizeof(S2)\n",
        "          lemmasz[0,1]+=sys.getsizeof(S01)\n",
        "          lemmasz[1,1]+=sys.getsizeof(S11)\n",
        "          lemmasz[2,1]+=sys.getsizeof(S21)\n",
        "          lemmasz[0,2]+=sys.getsizeof(S02)\n",
        "          lemmasz[1,2]+=sys.getsizeof(S12)\n",
        "          lemmasz[2,2]+=sys.getsizeof(S22)\n",
        "      else:\n",
        "          #Using snowball stemmer\n",
        "          S0=[stemmer.stem(i) for i in String]\n",
        "\n",
        "          String=' '.join(S0)\n",
        "\n",
        "      return String"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmo5lPkbjMJw"
      },
      "source": [
        "A benchmark has also been written to show the difference in sizes and time taken when different stemmers and lemmatisers are used on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkh_twTZYPyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d17ceeef-aaba-459f-a06c-4bf47c9845de"
      },
      "source": [
        "# Benchmark Option Toggle\n",
        "bench=1\n",
        "\n",
        "## Benchmarking varible initialisation\n",
        "if bench:\n",
        "    import numpy as np\n",
        "    import time\n",
        "    stemsz=np.zeros(3)\n",
        "    stopsz=0\n",
        "    lemmasz=np.zeros((3,3))\n",
        "    orisz=0\n",
        "    stemdur=np.zeros(3,dtype='float64')\n",
        "    lemmadur=np.zeros((3,3),dtype='float64')\n",
        "    rmpuncsz=0\n",
        "\n",
        "    #Imports Porter Stemmer and Lancaster Stemmer for benchmarking\n",
        "    from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "    porter=PorterStemmer()\n",
        "    lancaster=LancasterStemmer()\n",
        "\n",
        "    #Important to do this or run spacy.cli.download('en') from command line manually\n",
        "    '''This code will have error if is not running with appropriate permissions -> Will need to test directly with colab\n",
        "        !python -m spacy download en\n",
        "        OR'''\n",
        "    #Imports spacy lemmatiser\n",
        "    import spacy\n",
        "    spacy.cli.download('en')\n",
        "    spacymod=spacy.load('en')\n",
        "    \n",
        "    #Imports Textblob lemmatiser\n",
        "    from textblob import TextBlob"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXHI_MZsraUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "239a0cf7-eb4c-4f2f-cf6d-e25ecbee2ebc"
      },
      "source": [
        "if bench:\n",
        "    for f in range(len(docfilename)):\n",
        "        print(\"[%3.1f%%] Now processing: %s\"%((f+1.0)/len(docfilename)*100,docfilename[f]))\n",
        "        processString(open(os.path.join('.',docfilename[f]),'r',encoding='utf-8-sig').read(),stopw)\n",
        "    print()\n",
        "    print(orisz)\n",
        "    print(rmpuncsz)\n",
        "    print(stopsz)\n",
        "    print(stemsz)\n",
        "    print(stemdur)\n",
        "    print(lemmasz)\n",
        "    print(lemmadur)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[33.3%] Now processing: 7945.txt\n",
            "[66.7%] Now processing: 5642.txt\n",
            "[100.0%] Now processing: 7942.txt\n",
            "\n",
            "543048\n",
            "526584\n",
            "353720\n",
            "[307096. 307072. 263848.]\n",
            "[0.38859606 0.57730627 0.41971993]\n",
            "[[306752. 306960. 306576.]\n",
            " [306744. 307000. 306568.]\n",
            " [263240. 264720. 263048.]]\n",
            "[[0.10349345 3.39737725 0.31074262]\n",
            " [0.09578085 3.26415825 0.29463887]\n",
            " [0.10128593 3.29457283 0.27698231]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK0Dx9NH9SaW"
      },
      "source": [
        "Benchmark turned off as it is not needed after this"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrnE0Q1c9PpK"
      },
      "source": [
        "bench=0"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoRM0V2YtolB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8e7f7dbc-35e3-41ac-b6e8-ae93740bebc3"
      },
      "source": [
        "# processString testing\n",
        "processString(\"tokenizing is a way to do shorten and small pieces of words\", stopw)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'token shorten small piec'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT8o1uOZuk-i"
      },
      "source": [
        "The following function \"tokenizer\" reads in the path and the docfilename list which contains the list of name of the text documents to process. It uses the processString function to process the string by removing the stop words, symbols, stemmer and lemmatization. The function returns all the tokens in a document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCbNICZc9tHN"
      },
      "source": [
        "def tokenizer(path,docfilename):\n",
        "  t=dict()\n",
        "  tchar=['','\\n','\\t','\\n\\t','\\t\\n']\n",
        "  os.chdir(path)\n",
        "  for index, d in enumerate(docfilename):\n",
        "    fpath=path+\"/\"+d\n",
        "    f=open(d)\n",
        "    a=f.read()\n",
        "    a=processString(a, stopw)\n",
        "    a=a.split('\\n')\n",
        "    a=' '.join(a).split(' ')\n",
        "    for i in a:\n",
        "      if i not in tchar:\n",
        "        if i not in t:\n",
        "          t[i]=list()\n",
        "        if index not in t[i]:\n",
        "          t[i].append(index)\n",
        "  return t\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPx25PemwdkR"
      },
      "source": [
        "The following \"sort_token\" function sorts the token in alphabetical order and subsequently sorts the values which are the document id or indices in the key which is the token. \n",
        "\n",
        "The mergeposting function merges two lists together by using pointers to each list and compares the values of the index in the two lists. If the two lists are equal, the value will be added to the postlist. If a list value is smaller than the other, its index will be increased. \n",
        "\n",
        "The \"intersect\" function will extract two lists from the dictionary with reference to two words. It produces the merge posting list for the two words.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w5gSarpwSGN"
      },
      "source": [
        "def sort_token(token):\n",
        "  sdict=dict()\n",
        "  skey=sorted(token)\n",
        "  for s in skey:\n",
        "    sdict[s]=sorted(token[s])\n",
        "  return sdict\n",
        "\n",
        "def mergeposting(list1,list2):\n",
        "  postlist=list()\n",
        "  i=0\n",
        "  j=0\n",
        "  len1=len(list1)\n",
        "  len2=len(list2)\n",
        "    \n",
        "  while i<len1 and j<len2:\n",
        "    #print(i,\" \",j)\n",
        "    if list1[i]==list2[j]:\n",
        "      postlist.append(list1[i])\n",
        "      i+=1\n",
        "      j+=1\n",
        "    else:\n",
        "      if list1[i]<list2[j]:\n",
        "        i+=1\n",
        "      else:\n",
        "        j+=1\n",
        "  return postlist\n",
        "\n",
        "def intersect(sdict,word1,word2):\n",
        "  postlist=list()\n",
        "  try:\n",
        "    list1=sdict[word1]\n",
        "    list2=sdict[word2]\n",
        "    postlist=mergeposting(list1,list2)\n",
        "  except:\n",
        "    print(\"words not valid\")\n",
        "  return postlist\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQyfP0wyapSG"
      },
      "source": [
        "The following section takes into consideration the organisation of dictionary as a string. \n",
        "\n",
        "The code cell immediately below is used for binary search of terms from the dictionary which is stored as a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BdV4MEpaoMo"
      },
      "source": [
        "## Binary search algorithm\n",
        "# string : the dictionary stored as string\n",
        "# arr : pointers to beginning of words in the string\n",
        "# l : lower bound (for search) initially defined as the beginning of string\n",
        "# r : upper bound (for search) initially defined as the end of string\n",
        "# word : the word that is to be searched\n",
        "\n",
        "def searchForTerm ( string, arr, l, r, word): \n",
        "    # Check base case \n",
        "    if r >= l: \n",
        "        mid = l + (r - l) // 2\n",
        "  \n",
        "        # If element is present at the middle itself \n",
        "        if string[arr[mid]:arr[mid+1]] == word: \n",
        "            return mid \n",
        "          \n",
        "        # If element is smaller than mid, then it  \n",
        "        # can only be present in left subarray \n",
        "        elif string[arr[mid]:arr[mid+1]] > word: \n",
        "            return searchForTerm(string, arr, l, mid-1, word) \n",
        "  \n",
        "        # Else the element can only be present  \n",
        "        # in right subarray \n",
        "        else: \n",
        "            return searchForTerm(string, arr, mid + 1, r, word) \n",
        "  \n",
        "    else: \n",
        "        # Element is not present in the array \n",
        "        return -1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rql49O90bmsa"
      },
      "source": [
        "The following cell is a new intersect function which finds the posting list for each of the words in the query and then merges them. Here the dictionary is a string and terms are searched through binary search as mentioned above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HacqRxL7bidC"
      },
      "source": [
        "## Function to find the posting list that is common to both the words being searched for\n",
        "# positionDict : Its a dictionary that contains the pointers to the words as keys\n",
        "# and postings as values\n",
        "# joinedKeys : dictionary stored as string\n",
        "# word1 : first word to be searched\n",
        "# word2 : second word to be searched\n",
        "\n",
        "def intersectNew(positionDict,joinedKeys,word1,word2):\n",
        "  postlist=list()\n",
        "  #getting the pointer to each term in the dictionary string\n",
        "  keyValues = [*positionDict]\n",
        "  l = 0\n",
        "  r = len(keyValues)-1\n",
        "  index1 = searchForTerm(joinedKeys, keyValues, l, r, word1)\n",
        "  index2 = searchForTerm(joinedKeys, keyValues, l, r, word2)\n",
        "  if (index1 and index2 > 0):\n",
        "    list1 = positionDict[keyValues[index1]]\n",
        "    list2 = positionDict[keyValues[index2]]\n",
        "  try:\n",
        "    postlist=mergeposting(list1,list2)\n",
        "  except:\n",
        "    print(\"words not valid\")\n",
        "  return postlist\n"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gv8UtwlFdXWR"
      },
      "source": [
        "The following 2 functions convert the dictionary into string and create a new dictionary item with (key,value) as (pointer,postings) for every word in the dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjEkfKfCcmmt"
      },
      "source": [
        "## converts dictionary into string by taking the key values of sorted token pairs\n",
        "# and joining them as string\n",
        "# sdict: sorted token pair\n",
        "def getJoinedStringOfDict(sdict):\n",
        "  return ''.join([*sdict])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qxoA0uZcmyl"
      },
      "source": [
        "## Finds the position of the word in the string using the counter, which gets updated\n",
        "# after every word is processed by adding the length of the word to it and the postings\n",
        "# are copied to the values of the pointers (keys). \n",
        "# The final format is : positionDict ==> (key, value):(pointer, posting list)\n",
        "\n",
        "def getPositionDict(sdict):\n",
        "  count = 0\n",
        "  positionDict = dict()\n",
        "  for i in [*sdict]:\n",
        "    positionDict[count] = sdict[i]\n",
        "    count = count+len(i)\n",
        "  \n",
        "  return positionDict"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaJ24i3AO6e1"
      },
      "source": [
        "The following cells are codes for blocked storage by grouping the terms in the string into of size $k$ where $k$ = 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKGJjdORGabY"
      },
      "source": [
        "## Binary search algorithm to find the block to which a word belongs\n",
        "# string : the dictionary stored as string with length of words in block manner\n",
        "# arr : pointers to beginning of blocks in the string\n",
        "# l : lower bound (for search) initially defined as the beginning of string\n",
        "# r : upper bound (for search) initially defined as the end of string\n",
        "# word : the word that is to be searched\n",
        "\n",
        "def getBlockID (string, arr, l, r, word): \n",
        "    # Check base case \n",
        "    if r >= l: \n",
        "        mid = l + (r - l) // 2\n",
        "        start = arr[mid]+2\n",
        "        end = start+(int(string[arr[mid]:arr[mid]+2])) \n",
        "        firstWord = string[start:end]\n",
        "\n",
        "        startNext = arr[mid+1]+2\n",
        "        endNext = startNext + (int(string[arr[mid+1]:arr[mid+1]+2]))\n",
        "        firstWordNext = string[startNext:endNext]\n",
        "        \n",
        "        #pdb.set_trace()\n",
        "        # If element is present at the middle itself \n",
        "        if (firstWord == word) or (word>firstWord and word<firstWordNext):\n",
        "            return mid \n",
        "          \n",
        "        # If element is smaller than mid, then it  \n",
        "        # can only be present in left subarray \n",
        "        elif firstWord > word:\n",
        "            return getBlockID(string, arr, l, mid-1, word) \n",
        "  \n",
        "        # Else the element can only be present  \n",
        "        # in right subarray \n",
        "        elif (word>firstWord and word>firstWordNext): \n",
        "            return getBlockID(string, arr, mid + 1, r, word) \n",
        "  \n",
        "    else: \n",
        "        # Element is not present in the array \n",
        "        return -1"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI309hLaGkNW"
      },
      "source": [
        "## After it is determined which block to get the information from\n",
        "# we find the position of the word in the block that matches the \n",
        "# query word\n",
        "# blockString: block string containing length of the words and words\n",
        "# wordToTest : query word\n",
        "# returns the position of the word\n",
        "\n",
        "def getPositionOfWord (blockString, wordToTest):\n",
        "    end = 0\n",
        "    for i in range(4):\n",
        "        start = end+2\n",
        "        end = start+(int(blockString[end:end+2]))\n",
        "        #pdb.set_trace()\n",
        "        word = blockString[start:end]\n",
        "        if (word == wordToTest):\n",
        "            return i \n",
        "    return -1"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl5BcL7nGrAO"
      },
      "source": [
        "## This function is to find the postings list of the word that is in the query\n",
        "# from the dictionary stored in blocked storage fashion\n",
        "# positionDictNew : Its a dictionary that contains the pointers to the blocks as keys\n",
        "# and postings of the block as values\n",
        "# joinedKeysNew : dictionary stored as string with their length\n",
        "# word: word to be searched\n",
        "import pdb\n",
        "def getPostList (positionDictNew, joinedKeysNew, word):\n",
        "    pointers = [*positionDictNew]\n",
        "    left = 0\n",
        "    right = len(pointers)-1\n",
        "    blockID = getBlockID(joinedKeysNew, pointers, left, right, word)\n",
        "    if blockID > 0:\n",
        "        blockString = joinedKeysNew[pointers[blockID]:pointers[blockID+1]]\n",
        "        positionOfWord = getPositionOfWord (blockString, word)\n",
        "        #print(blockString)\n",
        "        #print(positionOfWord)\n",
        "        #pdb.set_trace()\n",
        "\n",
        "        if positionOfWord > 0 :\n",
        "            postList = positionDictNew[pointers[blockID]][positionOfWord]\n",
        "            return postList\n",
        "        else:\n",
        "            return -1\n",
        "    else:\n",
        "        return -1"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_yzXGHRGvln"
      },
      "source": [
        "## Function to find the posting list that is common to both the words being searched for\n",
        "# This is for the blocked storage type\n",
        "# positionDictNew : Its a dictionary that contains the pointers to the blocks as keys\n",
        "# and postings of the block as values\n",
        "# joinedKeysNew : dictionary stored as string with their length\n",
        "# word1 : first word to be searched\n",
        "# word2 : second word to be searched\n",
        "\n",
        "def intersectNewLength(positionDictNew, joinedKeysNew, word1, word2):\n",
        "    postlist = list()\n",
        "    list1 = getPostList(positionDictNew, joinedKeysNew, word1)\n",
        "    list2 = getPostList(positionDictNew, joinedKeysNew, word2)\n",
        "    \n",
        "    try:\n",
        "        postlist=mergeposting(list1,list2)\n",
        "    except:\n",
        "        print(\"words not valid\")\n",
        "    return postlist"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC2ov_qPPizt"
      },
      "source": [
        "## converts dictionary into string along with length of each word by taking the \n",
        "# key values of sorted token pairs and joining them as string with length \n",
        "# sdict: sorted token pair\n",
        "# returns string. eg. stringDict = ['hi', 'bye', 'take', 'care']\n",
        "# output --> 02hi03bye04take04care\n",
        "\n",
        "def getJoinedLengthStringOfDict(sdict):\n",
        "  stringDict = [*sdict]\n",
        "  listOfStrings = list()\n",
        "  for i in range(len(stringDict)):\n",
        "    if (len(stringDict[i])>9):\n",
        "      lengthOfString = str(len(stringDict[i]))\n",
        "    else:\n",
        "      lengthOfString = '0'+str(len(stringDict[i]))\n",
        "    listOfStrings.append(lengthOfString+stringDict[i])\n",
        "  return ''.join(listOfStrings)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MupLoGs4PjwG"
      },
      "source": [
        "## Finds the position of the block (of size 4) in the string using the counter, which gets updated\n",
        "# after every word is processed by adding the length of the block to it and the postings\n",
        "# are copied (for the whole block which are later retrieved for specific word while testing) \n",
        "# to the values of the pointers (keys). \n",
        "# The final format is : positionDict ==> (key, value):(pointer to block, posting list of block)\n",
        "\n",
        "def getPositionDictNew(sdict):\n",
        "  count = 0\n",
        "  k = 4   #4 block encoding \n",
        "  positionDictNew = dict()\n",
        "  sdictKeys = [*sdict]\n",
        "  for i in range(len(sdictKeys)):\n",
        "    if i%k == 0:\n",
        "      try:\n",
        "        positionDictNew[count] = [sdict[sdictKeys[i+d]] for d in range(k)]\n",
        "      except:\n",
        "        positionDictNew[count] = [sdict[sdictKeys[i+d]] for d in range(len(sdictKeys)%k)]\n",
        "    count = count+(len(sdictKeys[i]))+2 \n",
        "  return positionDictNew"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzxnn2CZ1YDK"
      },
      "source": [
        "The retrieval function takes in the string of query, process the query by processString function and split it into words. It pair the words and uses the merge listing to produce merge posting list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP8sQClGwXzG"
      },
      "source": [
        "def retrieval(query,sdict,optimise):\n",
        "    query=processString(query,stopw)\n",
        "    splitquery=query.split(' ')\n",
        "    \n",
        "    if optimise == 1:\n",
        "        #Get dictionary as string\n",
        "        joinedString = getJoinedStringOfDict(sdict)\n",
        "        #Get dictionary of pointers and postings as (key,value)\n",
        "        positionDict = getPositionDict(sdict)\n",
        "\n",
        "    elif optimise == 2:  \n",
        "        #Get dictionary as string with length of word before each word\n",
        "        joinedStringLength = getJoinedLengthStringOfDict(sdict)\n",
        "        #Get dictionary of pointers to blocks and postings of block as (key, values)\n",
        "        positionDictNew = getPositionDictNew(sdict)\n",
        "        \n",
        "    \n",
        "    lensplit=len(splitquery)\n",
        "    \n",
        "    if lensplit>1:\n",
        "        mergepost=None\n",
        "        for i in range(lensplit-1):\n",
        "            if optimise == 1:\n",
        "                #Get from the intersect function with dictionary as string storage\n",
        "                postlist = intersectNew(positionDict, joinedString, splitquery[i], splitquery[i+1])\n",
        "            elif optimise == 2:\n",
        "                #Get from the intersect function with blocked storage\n",
        "                postlist = intersectNewLength(positionDictNew, joinedStringLength, splitquery[i], splitquery[i+1])\n",
        "            else:\n",
        "                #Get from the standard intersect function\n",
        "                postlist=intersect(sdict,splitquery[i],splitquery[i+1])\n",
        "            \n",
        "            \n",
        "            if mergepost is not None:\n",
        "                mergepost=mergeposting(mergepost,postlist)\n",
        "            else:\n",
        "                mergepost=postlist\n",
        "    else:\n",
        "        pass\n",
        "    return mergepost  "
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37aNK8q-2Ktr"
      },
      "source": [
        "The following is a test for the retrieval system\n",
        "It builds the sorted token database from the dataset. Then uses the retrieval function to serve the query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDBEQCXE_LBo"
      },
      "source": [
        "# Test\n",
        "token=tokenizer('/content/gdrive/'+path,docfilename)\n",
        "stoken=sort_token(token)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfZPzTadSOMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8804e7-45a8-4264-fe73-16d550a3d980"
      },
      "source": [
        "# toggle between 0 , 1 and 2 for using the \"dictionary as string\" memory optimisation\n",
        "# 1 for optimisation with dictionary as string\n",
        "# 2 for optimisation with dictionary as string and blocked storage with k = 4\n",
        "# 0 for using the standard method\n",
        "optimise = 2\n",
        "docindex=retrieval(\"Border control\",stoken,optimise)\n",
        "docindex"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-pvkUu8ZFyc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32acc70c-6b24-4a37-b5a7-9c769bc67934"
      },
      "source": [
        "d=list()\n",
        "for i in docindex:\n",
        "  d.append(docfilename[i])\n",
        "print(\"Text document found: \",d)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text document found:  ['5642.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}